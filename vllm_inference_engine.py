# -*- coding: utf-8 -*-
"""vLLM Inference Engine

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mky2NhCqjAe-5pmGdPwaCGMEMkLL50y8
"""

! pip install vllm

from vllm import LLM

prompts = ["Hello, my name is", "The capital of France is"]  # Sample prompts.
llm = LLM(model="gpt2")  # Create an LLM.
outputs = llm.generate(prompts)  # Generate texts from the prompts.

"""# To use vLLM for online serving, you can start an OpenAI API-compatible server via:


"""

!curl ipv4.icanhazip.com

! python -m vllm.entrypoints.openai.api_server --host 127.0.0.1 --port 8888 --model bigscience/bloomz-560m & npx localtunnel --port 8888

!curl https://wise-rats-search.loca.lt/v1/completions -H "Content-Type: application/json" -d '{"model": "bigscience/bloomz-560m","prompt": "San Francisco is a","max_tokens": 7,"temperature": 0}'