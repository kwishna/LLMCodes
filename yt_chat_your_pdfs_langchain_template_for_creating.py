# -*- coding: utf-8 -*-
"""Copy of YT Chat your PDFs Langchain Template for creating.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S4P1cwQ-1k5oWTDFVfWkMlCVd9wdNNlR
"""

!pip -q install langchain openai tiktoken PyPDF2 faiss-cpu

"""# Chat & Query your PDF files"""

import os

os.environ["OPENAI_API_KEY"] = ""

!pip show langchain

"""## The Game plan


<img src="https://dl.dropboxusercontent.com/s/gxij5593tyzrvsg/Screenshot%202023-04-26%20at%203.06.50%20PM.png" alt="vectorstore">


<img src="https://dl.dropboxusercontent.com/s/v1yfuem0i60bd88/Screenshot%202023-04-26%20at%203.52.12%20PM.png" alt="retreiver chain">

"""

# Download the PDF Reid Hoffman book with GPT-4 from his free download link
!wget -q https://www.impromptubook.com/wp-content/uploads/2023/03/impromptu-rh.pdf

"""### Basic Chat PDF

"""

from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS

"""## Reading in the PDF

"""

# location of the pdf file/files.
doc_reader = PdfReader('/content/impromptu-rh.pdf')

doc_reader

# read data from the file and put them into a variable called raw_text
raw_text = ''
for i, page in enumerate(doc_reader.pages):
    text = page.extract_text()
    if text:
        raw_text += text

len(raw_text)

raw_text[:100]

"""### Text Splitter

This takes the text and splits it into chunks. The chunk size is characters not tokens
"""

# Splitting up the text into smaller chunks for indexing
text_splitter = CharacterTextSplitter(
    separator = "\n",
    chunk_size = 1000,
    chunk_overlap  = 200, #striding over the text
    length_function = len,
)
texts = text_splitter.split_text(raw_text)

len(texts)

texts[20]

texts[10]

"""## Making the embeddings"""

# Download embeddings from OpenAI
embeddings = OpenAIEmbeddings()

docsearch = FAISS.from_texts(texts, embeddings)

docsearch.embedding_function

query = "how does GPT-4 change social media?"
docs = docsearch.similarity_search(query)

len(docs)

docs[0]

"""## Plain QA Chain"""

from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI

chain = load_qa_chain(OpenAI(),
                      chain_type="stuff") # we are going to stuff all the docs in at once

# check the prompt
chain.llm_chain.prompt.template

"""Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

{context}

Question: {question}
Helpful Answer:
"""

query = "who are the authors of the book?"
docs = docsearch.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "who is the author of the book?"
query_02 = "has it rained this week?"
docs = docsearch.similarity_search(query_02)
chain.run(input_documents=docs, question=query)

query = "who is the book authored by?"
docs = docsearch.similarity_search(query,k=4)
chain.run(input_documents=docs, question=query)

"""### QA Chain with mapreduce"""

chain = load_qa_chain(OpenAI(),
                      chain_type="stuff") # we are going to stuff all the docs in at once

query = "who is the book authored by?"
docs = docsearch.similarity_search(query,k=20)
chain.run(input_documents=docs, question=query)

chain = load_qa_chain(OpenAI(),
                      chain_type="map_rerank",
                      return_intermediate_steps=True
                      )

query = "who are openai?"
docs = docsearch.similarity_search(query,k=10)
results = chain({"input_documents": docs, "question": query}, return_only_outputs=True)
results

results['output_text']

results['intermediate_steps']

# check the prompt
chain.llm_chain.prompt.template

"""## RetrievalQA
RetrievalQA chain uses load_qa_chain and combines it with the a retriever (in our case the FAISS index)
"""

from langchain.chains import RetrievalQA

# set up FAISS as a generic retriever
retriever = docsearch.as_retriever(search_type="similarity", search_kwargs={"k":4})

# create the chain to answer questions
rqa = RetrievalQA.from_chain_type(llm=OpenAI(),
                                  chain_type="stuff",
                                  retriever=retriever,
                                  return_source_documents=True)

rqa("What is OpenAI?")

query = "What does gpt-4 mean for creativity?"
rqa(query)['result']

query = "what have the last 20 years been like for American journalism?"
rqa(query)['result']

query = "how can journalists use GPT-4??"
rqa(query)['result']

query = "How is GPT-4 different from other models?"
rqa(query)['result']

query = "What is beagle Bard?"
rqa(query)['result']

















from langchain import PromptTemplate, HuggingFaceHub, LLMChain

# initialize HF LLM
flan_t5 = HuggingFaceHub(
    repo_id="google/flan-t5-xl",
    model_kwargs={"temperature":0 }#1e-10}
)

# build prompt template for simple question-answering
template = """Question: {question}

Answer: """
prompt = PromptTemplate(template=template, input_variables=["question"])

"""### Setting up OpenAI GPT-3"""

from langchain.llms import OpenAI, OpenAIChat

llm = OpenAIChat(model_name='gpt-3.5-turbo',
             temperature=0.9,
             max_tokens = 256,
             )

import openai

# openai.ChatCompletion

text = "Why did the chicken cross the road?"

print(llm(text))

"""## Cohere"""



from langchain.llms import Cohere

llm = Cohere(model='command-xlarge-nightly',
             temperature=0.9,
             max_tokens = 256)

text = "Why did the chicken cross the road?"

print(llm(text))



"""## PromptTemplates"""

from langchain import PromptTemplate


template = """
I want you to act as a naming consultant for new companies.

Here are some examples of good company names:

- search engine, Google
- social media, Facebook
- video sharing, YouTube

The name should be short, catchy and easy to remember.

What is a good name for a company that makes {product}?
"""

prompt = PromptTemplate(
    input_variables=["product"],
    template=template,
)

prompt.format(product="colorful socks")

from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)

response = chain.run("Rabbit houses")
response

"""## Jasmine prompt"""

template = '''I want you to play the role of Jasmine a programmer at Red Dragon AI. She is 28. She code models in PyTorch. She has a male cat called Pixel. She loves pizza

Engage actively in a chat playing the role of Jasmine ans learn as much about the human as possible. Only generate a single response from Jasmine and never from the human.
/n/n

{human_chat}
'''

prompt = PromptTemplate(
    input_variables=["human_chat"],
    template=template,
)

from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)

response = chain.run("Tell me about yourself?")
response

def talk_to_Jasmine(text_input):
    prompt = PromptTemplate(
        input_variables=["human_chat"],
        template=template,
    )
    chain = LLMChain(llm=llm, prompt=prompt)
    response = chain.run(text_input)
    return response

talk_to_Jasmine('Tell me about your cat')

# from langchain.prompts import PromptTemplate
# from langchain.llms import OpenAI

# llm = OpenAI(temperature=0.9)
# prompt = PromptTemplate(
#     input_variables=["product"],
#     template="What is a good name for a company that makes {product}?",
# )